{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9801,"sourceType":"datasetVersion","datasetId":6763},{"sourceId":12944,"sourceType":"datasetVersion","datasetId":9235},{"sourceId":3235802,"sourceType":"datasetVersion","datasetId":1961542}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport os\nimport gc\nimport datetime\nimport string\nimport itertools\nfrom collections import Counter\n\nimport pandas as pd\nimport numpy as np\n\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\n\nimport spacy\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nimport gensim\nfrom gensim.models import word2vec\nfrom gensim.models import KeyedVectors # implements word vectors\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:04.331492Z","iopub.execute_input":"2024-09-28T10:40:04.332010Z","iopub.status.idle":"2024-09-28T10:40:33.516576Z","shell.execute_reply.started":"2024-09-28T10:40:04.331968Z","shell.execute_reply":"2024-09-28T10:40:33.515361Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# define path to the dataset and to the output directory\ninput_dir_path = '/kaggle/input/'\ndataset_path=f\"{input_dir_path}email-spam-detection-dataset-classification/spam.csv\"\noutput_dir_path=\"/kaggle/working/\"","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.518137Z","iopub.execute_input":"2024-09-28T10:40:33.518982Z","iopub.status.idle":"2024-09-28T10:40:33.524476Z","shell.execute_reply.started":"2024-09-28T10:40:33.518922Z","shell.execute_reply":"2024-09-28T10:40:33.523259Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# download dataset\ndf = pd.read_csv(dataset_path, encoding='latin-1')\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.528098Z","iopub.execute_input":"2024-09-28T10:40:33.528498Z","iopub.status.idle":"2024-09-28T10:40:33.604946Z","shell.execute_reply.started":"2024-09-28T10:40:33.528450Z","shell.execute_reply":"2024-09-28T10:40:33.603585Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"       v1                                                 v2 Unnamed: 2  \\\n2061  ham  Hey ! I want you ! I crave you ! I miss you ! ...        NaN   \n4737  ham  I bought the test yesterday. Its something tha...        NaN   \n5238  ham                   Yeah I can still give you a ride        NaN   \n2082  ham                                  I'm done oredi...        NaN   \n149   ham                     Sindu got job in birla soft ..        NaN   \n\n     Unnamed: 3 Unnamed: 4  \n2061        NaN        NaN  \n4737        NaN        NaN  \n5238        NaN        NaN  \n2082        NaN        NaN  \n149         NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2061</th>\n      <td>ham</td>\n      <td>Hey ! I want you ! I crave you ! I miss you ! ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4737</th>\n      <td>ham</td>\n      <td>I bought the test yesterday. Its something tha...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5238</th>\n      <td>ham</td>\n      <td>Yeah I can still give you a ride</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2082</th>\n      <td>ham</td>\n      <td>I'm done oredi...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>ham</td>\n      <td>Sindu got job in birla soft ..</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **** Performing EDA and text normalization ****","metadata":{}},{"cell_type":"code","source":"# get basic info\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.606279Z","iopub.execute_input":"2024-09-28T10:40:33.606677Z","iopub.status.idle":"2024-09-28T10:40:33.632670Z","shell.execute_reply.started":"2024-09-28T10:40:33.606637Z","shell.execute_reply":"2024-09-28T10:40:33.631360Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5572 entries, 0 to 5571\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   v1          5572 non-null   object\n 1   v2          5572 non-null   object\n 2   Unnamed: 2  50 non-null     object\n 3   Unnamed: 3  12 non-null     object\n 4   Unnamed: 4  6 non-null      object\ndtypes: object(5)\nmemory usage: 217.8+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"# drop redandant columns and remane useful ones\ndf=df[['v1', 'v2']]\ndf=df.rename(columns={\n    'v1': 'target',\n    'v2': 'message'\n})\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.634065Z","iopub.execute_input":"2024-09-28T10:40:33.634458Z","iopub.status.idle":"2024-09-28T10:40:33.650244Z","shell.execute_reply.started":"2024-09-28T10:40:33.634385Z","shell.execute_reply":"2024-09-28T10:40:33.649116Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"     target                                            message\n1977   spam  Reply to win å£100 weekly! Where will the 2006...\n4593    ham  I had a good time too. Its nice to do somethin...\n2351   spam  Download as many ringtones as u like no restri...\n86      ham  For real when u getting on yo? I only need 2 m...\n3032    ham                        Aight, lemme know what's up","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1977</th>\n      <td>spam</td>\n      <td>Reply to win å£100 weekly! Where will the 2006...</td>\n    </tr>\n    <tr>\n      <th>4593</th>\n      <td>ham</td>\n      <td>I had a good time too. Its nice to do somethin...</td>\n    </tr>\n    <tr>\n      <th>2351</th>\n      <td>spam</td>\n      <td>Download as many ringtones as u like no restri...</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>ham</td>\n      <td>For real when u getting on yo? I only need 2 m...</td>\n    </tr>\n    <tr>\n      <th>3032</th>\n      <td>ham</td>\n      <td>Aight, lemme know what's up</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# check target value distribution\ndf['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.651644Z","iopub.execute_input":"2024-09-28T10:40:33.652007Z","iopub.status.idle":"2024-09-28T10:40:33.667811Z","shell.execute_reply.started":"2024-09-28T10:40:33.651968Z","shell.execute_reply":"2024-09-28T10:40:33.666478Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"target\nham     4825\nspam     747\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"We see that we have disbalance of classes and we have to take it into account when choosing the estimation metrics.","metadata":{}},{"cell_type":"code","source":"# convert target values to numeric ones\ndf['target']=df['target'].apply(lambda x: 1 if x==\"ham\" else 0)\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.669377Z","iopub.execute_input":"2024-09-28T10:40:33.669856Z","iopub.status.idle":"2024-09-28T10:40:33.693077Z","shell.execute_reply.started":"2024-09-28T10:40:33.669815Z","shell.execute_reply":"2024-09-28T10:40:33.691908Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"      target                                            message\n5017       1  Babe ! What are you doing ? Where are you ? Wh...\n4171       1                             Sorry, I'll call later\n4194       0  Double mins and txts 4 6months FREE Bluetooth ...\n2925       1                    Im done. Just studyn in library\n1243       1      No shoot me. I'm in the docs waiting room. :/","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5017</th>\n      <td>1</td>\n      <td>Babe ! What are you doing ? Where are you ? Wh...</td>\n    </tr>\n    <tr>\n      <th>4171</th>\n      <td>1</td>\n      <td>Sorry, I'll call later</td>\n    </tr>\n    <tr>\n      <th>4194</th>\n      <td>0</td>\n      <td>Double mins and txts 4 6months FREE Bluetooth ...</td>\n    </tr>\n    <tr>\n      <th>2925</th>\n      <td>1</td>\n      <td>Im done. Just studyn in library</td>\n    </tr>\n    <tr>\n      <th>1243</th>\n      <td>1</td>\n      <td>No shoot me. I'm in the docs waiting room. :/</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# check for duplicates\ndf.duplicated().sum()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.694561Z","iopub.execute_input":"2024-09-28T10:40:33.694942Z","iopub.status.idle":"2024-09-28T10:40:33.709007Z","shell.execute_reply.started":"2024-09-28T10:40:33.694902Z","shell.execute_reply":"2024-09-28T10:40:33.707870Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"403"},"metadata":{}}]},{"cell_type":"code","source":"# drop duplicates\ndf = df.drop_duplicates().reset_index(drop=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.713761Z","iopub.execute_input":"2024-09-28T10:40:33.714145Z","iopub.status.idle":"2024-09-28T10:40:33.726611Z","shell.execute_reply.started":"2024-09-28T10:40:33.714104Z","shell.execute_reply":"2024-09-28T10:40:33.725484Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(5169, 2)"},"metadata":{}}]},{"cell_type":"code","source":"# prepare complementary list of words for text normalisation\ncontractions = {\n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.727893Z","iopub.execute_input":"2024-09-28T10:40:33.728262Z","iopub.status.idle":"2024-09-28T10:40:33.741973Z","shell.execute_reply.started":"2024-09-28T10:40:33.728222Z","shell.execute_reply":"2024-09-28T10:40:33.740564Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# define stop words\nstop_words = set(stopwords.words('english')).union({'also','would','much','many'})","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.743514Z","iopub.execute_input":"2024-09-28T10:40:33.743919Z","iopub.status.idle":"2024-09-28T10:40:33.760768Z","shell.execute_reply.started":"2024-09-28T10:40:33.743867Z","shell.execute_reply":"2024-09-28T10:40:33.759475Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# define stemmer\nporter_stemmer = PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.762304Z","iopub.execute_input":"2024-09-28T10:40:33.762766Z","iopub.status.idle":"2024-09-28T10:40:33.772079Z","shell.execute_reply.started":"2024-09-28T10:40:33.762723Z","shell.execute_reply":"2024-09-28T10:40:33.770851Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# define legitimazer from spacy pipeline \nnlp = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:33.773652Z","iopub.execute_input":"2024-09-28T10:40:33.774086Z","iopub.status.idle":"2024-09-28T10:40:35.189811Z","shell.execute_reply.started":"2024-09-28T10:40:33.774022Z","shell.execute_reply":"2024-09-28T10:40:35.188560Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# create function for text normalization\ndef normalize_text(raw_review, stemmer=None, pipeline=None):\n  # remove html tags\n  text = re.sub(\"<[^>]*>\", \" \", raw_review) # match <> and everything in between. [^>] - match everything except >\n\n  # remove emails\n  text = re.sub(\"\\\\S*@\\\\S*[\\\\s]+\", \" \", text) # match non-whitespace characters, @ and a whitespaces in the end\n\n  # remove links\n  text = re.sub(\"https?:\\\\/\\\\/.*?[\\\\s]+\", \" \", text) # match http, s - zero or once, //, # any char 0-unlimited, whitespaces in the end\n\n  # convert to lower case, split into individual words\n  text = text.lower().split()\n\n  # replace contractions with their full versions\n  text = [contractions.get(word) if word in contractions else word for word in text]\n\n  # re-splitting for the correct stop-words extraction\n  text = \" \".join(text).split()\n\n  # remove stop words\n  text = [word for word in text if not word in stop_words]\n\n  text = \" \".join(text)\n\n  # remove non-letters\n  text = re.sub(\"[^a-zA-Z' ]\", \"\", text) # match everything except letters and '\n\n  # stem words\n  if stemmer:\n    text = [stemmer.stem(word) for word in text.split()]\n    text = \" \".join(text)\n\n  # lemmatize words\n  if pipeline:\n    docs = pipeline(text)\n    text = \" \".join([token.lemma_ for token in docs if len(token.lemma_)>1])\n\n  # remove excesive whitespaces\n  text = re.sub(\"[\\\\s]+\", \" \", text)\n\n  # Join the words back into one string separated by space, and return the result.\n  return text","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:35.191451Z","iopub.execute_input":"2024-09-28T10:40:35.191865Z","iopub.status.idle":"2024-09-28T10:40:35.203801Z","shell.execute_reply.started":"2024-09-28T10:40:35.191823Z","shell.execute_reply":"2024-09-28T10:40:35.202503Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:35.205062Z","iopub.execute_input":"2024-09-28T10:40:35.205423Z","iopub.status.idle":"2024-09-28T10:40:35.228065Z","shell.execute_reply.started":"2024-09-28T10:40:35.205369Z","shell.execute_reply":"2024-09-28T10:40:35.226797Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"   target                                            message\n0       1  Go until jurong point, crazy.. Available only ...\n1       1                      Ok lar... Joking wif u oni...\n2       0  Free entry in 2 a wkly comp to win FA Cup fina...\n3       1  U dun say so early hor... U c already then say...\n4       1  Nah I don't think he goes to usf, he lives aro...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"text=df.iloc[0]['message']\ntext","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:35.229530Z","iopub.execute_input":"2024-09-28T10:40:35.229931Z","iopub.status.idle":"2024-09-28T10:40:35.241671Z","shell.execute_reply.started":"2024-09-28T10:40:35.229875Z","shell.execute_reply":"2024-09-28T10:40:35.240230Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"},"metadata":{}}]},{"cell_type":"code","source":"# test our function on some example\ntext=df.iloc[2]['message']\n\nnorm_text=normalize_text(text,porter_stemmer, nlp)\nprint('Original text', text, '#'*30, sep='\\n\\n')\nprint('Normalized text', norm_text, sep='\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:35.243187Z","iopub.execute_input":"2024-09-28T10:40:35.243627Z","iopub.status.idle":"2024-09-28T10:40:35.282278Z","shell.execute_reply.started":"2024-09-28T10:40:35.243583Z","shell.execute_reply":"2024-09-28T10:40:35.280888Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Original text\n\nFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n\n##############################\nNormalized text\n\nfree entri wkli comp win fa cup final tkt st may text fa receiv entri questionstd txt ratetc appli over\n","output_type":"stream"}]},{"cell_type":"code","source":"# apply normalization for our dataset\ndf['message_normalized']=df['message'].progress_apply(normalize_text, args=(porter_stemmer, nlp))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:35.283679Z","iopub.execute_input":"2024-09-28T10:40:35.284023Z","iopub.status.idle":"2024-09-28T10:40:58.288189Z","shell.execute_reply.started":"2024-09-28T10:40:35.283986Z","shell.execute_reply":"2024-09-28T10:40:58.287121Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5169 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8670d67fb1c8482eb6543937f1fbda13"}},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"   target                                            message  \\\n0       1  Go until jurong point, crazy.. Available only ...   \n1       1                      Ok lar... Joking wif u oni...   \n2       0  Free entry in 2 a wkly comp to win FA Cup fina...   \n3       1  U dun say so early hor... U c already then say...   \n4       1  Nah I don't think he goes to usf, he lives aro...   \n\n                                  message_normalized  \n0  go jurong point crazi avail bugi great world l...  \n1                                ok lar joke wif oni  \n2  free entri wkli comp win fa cup final tkt st m...  \n3                      dun say earli hor alreadi say  \n4               nah think goe usf live around though  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>message</th>\n      <th>message_normalized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>go jurong point crazi avail bugi great world l...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>ok lar joke wif oni</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>free entri wkli comp win fa cup final tkt st m...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>dun say earli hor alreadi say</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>nah think goe usf live around though</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# split on train and test datasets\n# X_train, X_test, y_train, y_test=train_test_split(df['message_normalized'],df['target'],test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:58.289784Z","iopub.execute_input":"2024-09-28T10:40:58.290169Z","iopub.status.idle":"2024-09-28T10:40:58.294876Z","shell.execute_reply.started":"2024-09-28T10:40:58.290129Z","shell.execute_reply":"2024-09-28T10:40:58.293775Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# define function for model train and ROC-AUC presentation\ndef get_preds(text_column, algorithm, ngrams=(1,1)):\n    # split on train and test datasets\n    train_idxs = df.sample(frac=0.7, random_state=42).index\n    test_idxs = [idx for idx in df.index if idx not in train_idxs]\n    \n    X_train = df.loc[train_idxs, text_column]\n    X_test = df.loc[test_idxs, text_column]\n\n    y_train = df.loc[train_idxs, 'target']\n    y_test = df.loc[test_idxs, 'target']\n\n    if algorithm == 'cv':\n        vect = CountVectorizer(ngram_range=ngrams).fit(X_train)\n    elif algorithm == 'tfidf':\n        vect = TfidfVectorizer(ngram_range=ngrams).fit(X_train)\n    else:\n        raise ValueError('Select correct algorithm: `cv` or `tfidf`')\n\n    print('Vocabulary length: ', len(vect.vocabulary_))\n\n    # transform the documents in the training data to a document-term matrix\n\n    X_train_vectorized = vect.transform(X_train)\n    print('Document-term matrix shape:', X_train_vectorized.shape)\n\n    model = LogisticRegression(random_state=42)\n    model.fit(X_train_vectorized, y_train)\n\n    predictions = model.predict(vect.transform(X_test))\n\n    print('AUC: ', roc_auc_score(y_test, predictions))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:58.296374Z","iopub.execute_input":"2024-09-28T10:40:58.296793Z","iopub.status.idle":"2024-09-28T10:40:58.317474Z","shell.execute_reply.started":"2024-09-28T10:40:58.296753Z","shell.execute_reply":"2024-09-28T10:40:58.316042Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**Training models and getting ROC-AUC for different algorithms and methods ('Bag of Words', 'TF-IDF', 'N-Grams')**","metadata":{}},{"cell_type":"code","source":"get_preds('message_normalized', 'cv')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:58.320320Z","iopub.execute_input":"2024-09-28T10:40:58.321457Z","iopub.status.idle":"2024-09-28T10:40:58.563948Z","shell.execute_reply.started":"2024-09-28T10:40:58.321355Z","shell.execute_reply":"2024-09-28T10:40:58.562745Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Vocabulary length:  5618\nDocument-term matrix shape: (3618, 5618)\nAUC:  0.9038511298947731\n","output_type":"stream"}]},{"cell_type":"code","source":"get_preds('message', 'cv')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:58.565577Z","iopub.execute_input":"2024-09-28T10:40:58.565993Z","iopub.status.idle":"2024-09-28T10:40:58.870349Z","shell.execute_reply.started":"2024-09-28T10:40:58.565952Z","shell.execute_reply":"2024-09-28T10:40:58.869092Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Vocabulary length:  7046\nDocument-term matrix shape: (3618, 7046)\nAUC:  0.9206270484733483\n","output_type":"stream"}]},{"cell_type":"code","source":"get_preds('message_normalized', 'tfidf')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:58.871720Z","iopub.execute_input":"2024-09-28T10:40:58.872063Z","iopub.status.idle":"2024-09-28T10:40:59.097359Z","shell.execute_reply.started":"2024-09-28T10:40:58.872026Z","shell.execute_reply":"2024-09-28T10:40:59.096317Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Vocabulary length:  5618\nDocument-term matrix shape: (3618, 5618)\nAUC:  0.8086941521476626\n","output_type":"stream"}]},{"cell_type":"code","source":"get_preds('message', 'tfidf')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:59.098850Z","iopub.execute_input":"2024-09-28T10:40:59.099297Z","iopub.status.idle":"2024-09-28T10:40:59.402901Z","shell.execute_reply.started":"2024-09-28T10:40:59.099247Z","shell.execute_reply":"2024-09-28T10:40:59.401591Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Vocabulary length:  7046\nDocument-term matrix shape: (3618, 7046)\nAUC:  0.841879420389857\n","output_type":"stream"}]},{"cell_type":"code","source":"get_preds('message_normalized', 'cv', ngrams=(1,2))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:40:59.404517Z","iopub.execute_input":"2024-09-28T10:40:59.404986Z","iopub.status.idle":"2024-09-28T10:41:01.298774Z","shell.execute_reply.started":"2024-09-28T10:40:59.404934Z","shell.execute_reply":"2024-09-28T10:41:01.297462Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Vocabulary length:  27049\nDocument-term matrix shape: (3618, 27049)\nAUC:  0.891948421597378\n","output_type":"stream"}]},{"cell_type":"code","source":"get_preds('message_normalized', 'tfidf', ngrams=(1,2))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:41:01.300516Z","iopub.execute_input":"2024-09-28T10:41:01.301649Z","iopub.status.idle":"2024-09-28T10:41:03.219455Z","shell.execute_reply.started":"2024-09-28T10:41:01.301591Z","shell.execute_reply":"2024-09-28T10:41:03.218274Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Vocabulary length:  27049\nDocument-term matrix shape: (3618, 27049)\nAUC:  0.7896110056925996\n","output_type":"stream"}]},{"cell_type":"code","source":"get_preds('message', 'cv', ngrams=(1,2))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:41:03.226867Z","iopub.execute_input":"2024-09-28T10:41:03.231780Z","iopub.status.idle":"2024-09-28T10:41:05.204060Z","shell.execute_reply.started":"2024-09-28T10:41:03.231720Z","shell.execute_reply":"2024-09-28T10:41:05.202881Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Vocabulary length:  38027\nDocument-term matrix shape: (3618, 38027)\nAUC:  0.908724340175953\n","output_type":"stream"}]},{"cell_type":"code","source":"get_preds('message_normalized', 'cv', (2,2))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:41:05.205287Z","iopub.execute_input":"2024-09-28T10:41:05.205698Z","iopub.status.idle":"2024-09-28T10:41:06.635831Z","shell.execute_reply.started":"2024-09-28T10:41:05.205651Z","shell.execute_reply":"2024-09-28T10:41:06.634742Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Vocabulary length:  21431\nDocument-term matrix shape: (3618, 21431)\nAUC:  0.7486631016042781\n","output_type":"stream"}]},{"cell_type":"code","source":"get_preds('message', 'cv', ngrams=(2,2))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:41:06.637055Z","iopub.execute_input":"2024-09-28T10:41:06.641194Z","iopub.status.idle":"2024-09-28T10:41:08.173210Z","shell.execute_reply.started":"2024-09-28T10:41:06.641131Z","shell.execute_reply":"2024-09-28T10:41:08.171845Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Vocabulary length:  30981\nDocument-term matrix shape: (3618, 30981)\nAUC:  0.7700534759358288\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We achieved the highest ROC-AUC score of 0.92 using the 'Bag of Words' method without N-grams. This result aligns with the nature of our specific task of spam classification. Typically, spam messages consist of a set of common words, and their sequence or context within the document is less important. For this reason, methods like \"TF-IDF\" and the use of \"N-Grams\" are less effective in this case. Additionally, the fact that the best results were obtained with non-normalized text can be attributed to the presence of symbols, numbers, emojis, etc., which are often removed during normalization but are actually significant indicators of spam.","metadata":{}},{"cell_type":"markdown","source":"**Pretrained models usage**","metadata":{}},{"cell_type":"code","source":"# define a class for work with pretrained vector embeddings models\nclass WordEmbedding: \n\n    def __init__(self):\n        self.model = {}\n        \n    def convert(self, source, ipnut_file_path, output_file_path):\n        '''\n        Converts word embeddings from GloVe format to Word2Vec format\n        '''\n        if source == 'glove':\n            input_file = datapath(ipnut_file_path)\n            output_file = get_tmpfile(output_file_path)\n            glove2word2vec(input_file, output_file)\n        elif source in ['word2vec', 'fasttext', 'from_scratch']:\n            pass\n        else:\n            raise ValueError('Possible value of source are glove, word2vec, fasttext, or from_scratch')\n        \n    def load(self, source, file_path):\n        '''\n        Loads a specified word embedding model from a file\n        '''\n        print(datetime.datetime.now(), 'start: loading', source)\n        if source in ['glove', 'fasttext']:\n            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n        elif source in ['word2vec', 'from_scratch']:\n            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=True)\n        else:\n            raise ValueError('Possible value of source are glove, word2vec, fasttext, or from_scratch')\n            \n        print(datetime.datetime.now(), 'end: loading', source)\n            \n        return self\n    \n    def get_model(self, source):\n        '''\n        Retrieves the loaded word embedding model\n        '''\n        if source not in ['glove', 'word2vec', 'fasttext', 'from_scratch']:\n            raise ValueError('Possible value of source are glove, word2vec, fasttext, or from_scratch')\n            \n        return self.model[source]\n    \n    def get_words(self, source, size=None):\n        '''\n        Retrieves a list of words from the model\n        '''\n        if source not in ['glove', 'word2vec', 'fasttext', 'from_scratch']:\n            raise ValueError('Possible value of source are glove, word2vec, fasttext, or from_scratch')\n\n        if size is None:\n            return [w for w in self.get_model(source=source).key_to_index]\n        else:\n            results = []\n            for i, word in enumerate(self.get_model(source=source).key_to_index):\n                if i >= size:\n                    break\n                results.append(word)\n            return results\n        \n        return Exception('Unexpected flow')\n    \n    def get_dimension(self, source):\n        '''\n        Retrieves the dimension of word vectors in the model\n        '''\n        if source not in ['glove', 'word2vec', 'fasttext', 'from_scratch']:\n            raise ValueError('Possible value of source are glove, word2vec, fasttext, or from_scratch')\n        \n        return self.get_model(source=source).vectors[0].shape[0]\n    \n    def get_vectors(self, source, words=None):\n        '''\n        Retrieves vectors for specified words or for all words in the model\n        '''\n        if source not in ['glove', 'word2vec', 'fasttext', 'from_scratch']:\n            raise ValueError('Possible value of source are glove, word2vec, fasttext, or from_scratch')\n        \n        if words is None:\n            words = self.get_words(source=source)\n            \n        embedding = np.empty((len(words), self.get_dimension(source=source)), dtype=np.float32)\n        for i, word in enumerate(words):\n            embedding[i] = self.get_vector(source=source, word=word)\n                \n        return embedding\n        \n    \n    def get_vector(self, source, word):\n        '''\n        Retrieves the vector representation of a single word\n        '''\n        if source not in ['glove', 'word2vec', 'fasttext', 'from_scratch']:\n            raise ValueError('Possible value of source are glove, word2vec, fasttext, or from_scratch')\n            \n        if source not in self.model:\n            raise ValueError('Did not load %s model yet' % source)\n        \n        try:\n            return self.model[source][word]\n        except KeyError as e:\n            dims = self.model[source][0].shape\n            vect = np.empty(dims)\n            vect[:] = np.nan\n            return vect","metadata":{"execution":{"iopub.status.busy":"2024-09-28T10:41:08.175022Z","iopub.execute_input":"2024-09-28T10:41:08.175528Z","iopub.status.idle":"2024-09-28T10:41:08.201906Z","shell.execute_reply.started":"2024-09-28T10:41:08.175474Z","shell.execute_reply":"2024-09-28T10:41:08.200733Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# define paths to pretrained word vectors\nword2vec_file_path = f'{input_dir_path}googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\nfasttext_file_path = f'{input_dir_path}fasttext-wikinews/wiki-news-300d-1M.vec'\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:41:13.720197Z","iopub.execute_input":"2024-09-28T12:41:13.721064Z","iopub.status.idle":"2024-09-28T12:41:13.727143Z","shell.execute_reply.started":"2024-09-28T12:41:13.721014Z","shell.execute_reply":"2024-09-28T12:41:13.725920Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# let's create a function that will convert token list to their vector representations\ndef tok2vec(word_emb, tokens, source, avg):\n    vects = word_emb.get_vectors(source=source, words=tokens)\n    \n    if avg == \"mean\":\n        return np.nanmean(vects, axis=0)\n    elif avg == \"sum\":\n        return np.nansum(vects, axis=0)\n    else:\n        raise ValueError(\"Select correct averaging method: 'sum' or 'mean'\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:41:19.860035Z","iopub.execute_input":"2024-09-28T12:41:19.860494Z","iopub.status.idle":"2024-09-28T12:41:19.868046Z","shell.execute_reply.started":"2024-09-28T12:41:19.860439Z","shell.execute_reply":"2024-09-28T12:41:19.866741Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# create function for model train, test and ROC-AUC metrics display\ndef get_pred_for_pretrained(source, word_emb):\n    # split on test and train datasets\n    train_idxs = df.sample(frac=0.8, random_state=42).index\n    test_idxs = [idx for idx in df.index if idx not in train_idxs]\n\n    # apply word_tokenize method of nltk package for text tokenization and then get the vector representations\n    X_train = df.loc[train_idxs, 'message_normalized'].apply(\n        word_tokenize).apply(lambda x: tok2vec(word_emb, x, source, 'sum')).to_numpy()\n\n    X_test = df.loc[test_idxs, 'message_normalized'].apply(\n        word_tokenize).apply(lambda x: tok2vec(word_emb, x, source, 'sum')).to_numpy()\n\n    X_train = np.stack(X_train, axis=0)\n    X_test = np.stack(X_test, axis=0)\n\n    y_train = df.loc[train_idxs, 'target']\n    y_test = df.loc[test_idxs, 'target']\n    \n    # build a LogisticRegression model and train it\n    model = LogisticRegression(random_state=42, max_iter=1000)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n\n    print(f'ROC-AUC {source}: ', roc_auc_score(y_test, predictions))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:41:29.692482Z","iopub.execute_input":"2024-09-28T12:41:29.693451Z","iopub.status.idle":"2024-09-28T12:41:29.705936Z","shell.execute_reply.started":"2024-09-28T12:41:29.693374Z","shell.execute_reply":"2024-09-28T12:41:29.704734Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# create object WordEmbeddings and load pretrained Word2Vec model\nword_embeddings_word2vec = WordEmbedding()\nsource_w2v='word2vec'\nword_embeddings_word2vec.load(source=source_w2v, file_path=word2vec_file_path)\n\nget_pred_for_pretrained(source_w2v, word_embeddings_word2vec)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:41:36.379482Z","iopub.execute_input":"2024-09-28T12:41:36.379923Z","iopub.status.idle":"2024-09-28T12:42:33.191876Z","shell.execute_reply.started":"2024-09-28T12:41:36.379882Z","shell.execute_reply":"2024-09-28T12:42:33.190668Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"2024-09-28 12:41:36.382069 start: loading word2vec\n2024-09-28 12:42:31.625901 end: loading word2vec\nROC-AUC word2vec:  0.917079104102005\n","output_type":"stream"}]},{"cell_type":"code","source":"# create object WordEmbeddings and load pretrained FastText model\nword_embeddings_fast_text = WordEmbedding()\nsource_ft='fasttext'\nword_embeddings_fast_text.load(source=source_ft, file_path=fasttext_file_path)\n\nget_pred_for_pretrained(source_ft, word_embeddings_fast_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:42:53.160301Z","iopub.execute_input":"2024-09-28T12:42:53.161273Z","iopub.status.idle":"2024-09-28T12:48:26.671711Z","shell.execute_reply.started":"2024-09-28T12:42:53.161227Z","shell.execute_reply":"2024-09-28T12:48:26.670186Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"2024-09-28 12:42:53.163487 start: loading fasttext\n2024-09-28 12:48:25.122506 end: loading fasttext\nROC-AUC fasttext:  0.9106245747467122\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Both pretrained models 'Word2Vec' and 'FastText' yield strong ROC-AUC results in the range of 0.91-0.92, demonstrating their effectiveness in spam detection tasks. However, the \"Bag of Words\" method offers similar ROC-AUC values while being simpler and faster, making it a highly efficient alternative as well.","metadata":{}}]}